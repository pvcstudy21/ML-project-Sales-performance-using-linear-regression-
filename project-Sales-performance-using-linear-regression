# Importing Libraries

# for data manipulation
import numpy as np
import pandas as pd

# for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# for model training and model evaluation
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score

# for Linear regression assumptions
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import shapiro
from scipy import stats

# Data Gathering

df = pd.read_csv(r"C:\Users\Dell\Downloads\computed_insight_success_of_active_sellers.csv")
print(df)

# Exploratory Data Analysis (EDA)

print(df.shape)
print(df.columns)
print(df.describe()) 
print(df.info())
print(df.isna().sum())

# checking if the outliers are pressent

print(sns.boxplot(df["rating"]))

# Feature Engineering

df = df.drop(["totalurgencycount","urgencytextrate","merchantid"],axis=1) 

# removing outliers

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3-Q1

LowerTail = Q1-1.5*IQR
UpperTail = Q3+1.5*IQR

show_outliers = (df<LowerTail)|(df>UpperTail)
outlier_count = show_outliers.sum()
print(outlier_count)

# Remove outliers
df_cleaned = df[~show_outliers.any(axis=1)]                            # Remove rows containing any outliers in any column

# Feature Selection

# linearity 

print(df_cleaned.corr())

print(sns.heatmap(df_cleaned.corr(),annot=True))

# no multicolinearity 

df1 = df_cleaned.drop(["totalunitssold"],axis=1)

vif_df = pd.DataFrame()
vif_df["Features"] = df1.columns
print(vif_df)

vif_list = []
for i in range(df1.shape[1]):
    vif = variance_inflation_factor(df1.to_numpy(),i)
    vif_list.append(vif)
print(vif_list)

vif_df["VIF"] = vif_list
print(vif_df)

# remove strongly correleted collumns

df_cleaned = df_cleaned.drop(["averagediscount"],axis=1)

# Model Training

x = df_cleaned.drop(["totalunitssold"],axis=1)
y = df_cleaned["totalunitssold"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

lin_reg = LinearRegression()

lin_reg.fit(x_train,y_train)

y_train_pred = lin_reg.predict(x_train)

y_test_pred = lin_reg.predict(x_test)

# normality of residual

residual_train = y_train - y_train_pred

print(sns.kdeplot(residual_train,fill=True))

# hypothesis testing

p_val, stats = shapiro(residual_train)

if p_val >= 0.05:
    print(p_val)
    print(stats)
    print("Null hypothesis H0 is True")
    print("Data Is normally distributed")
    
else:
    print("Alternative Hypothesis is True")
    print("Data is not normally distributed")

# homoscedasticity

print(sns.scatterplot(residual_train))

# Model Evaluation

# for training
mse = mean_squared_error(y_train,y_train_pred)
print("Mean squared Error for training = ",mse)

mae = mean_absolute_error(y_train,y_train_pred)
print("Mean absolute Error for training = ",mae)

rmse = np.sqrt(mse)
print("Root Mean squared Error for training = ",rmse)

r2score = r2_score(y_train,y_train_pred)
print("R2 score for training = ",r2score)

# for testing
mse = mean_squared_error(y_test,y_test_pred)
print("Mean squared Error for testing = ",mse)

mae = mean_absolute_error(y_test,y_test_pred)
print("Mean absolute Error for testing = ",mae)

rmse = np.sqrt(mse)
print("Root Mean squared Error for testing = ",rmse)

r2score = r2_score(y_test,y_test_pred)
print("R2 score for testing = ",r2score)